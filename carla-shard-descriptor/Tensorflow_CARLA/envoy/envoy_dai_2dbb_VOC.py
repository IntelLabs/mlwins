#!/usr/bin/env python

# Copyright (c) 2019 Computer Vision Center (CVC) at the Universitat Autonoma de
# Barcelona (UAB).
#
# This work is licensed under the terms of the MIT license.
# For a copy, see <https://opensource.org/licenses/MIT>.

from cgi import test
import glob
import os
import sys

try:
    #sys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (
    #    sys.version_info.major,
    #    sys.version_info.minor,
    #    'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])
    sys.path.append(glob.glob('/home/inteluser/Desktop/Shenghong/CARLA_package/CARLA_0.9.11/PythonAPI/carla/dist/carla-0.9.11-py3.7-linux-x86_64.egg')[0])
except IndexError:
    pass

import carla

import random
random.seed(1)
import time

import argparse

import math
import queue
import numpy as np
import cv2
from pascal_voc_writer import Writer
import subprocess
        
def build_projection_matrix(w, h, fov):
    focal = w / (2.0 * np.tan(fov * np.pi / 360.0))
    K = np.identity(3)
    K[0, 0] = K[1, 1] = focal
    K[0, 2] = w / 2.0
    K[1, 2] = h / 2.0
    return K

def get_image_point(loc, K, w2c):
    # Calculate 2D projection of 3D coordinate

    # Format the input coordinate (loc is a carla.Position object)
    point = np.array([loc.x, loc.y, loc.z, 1])
    # transform to camera coordinates
    point_camera = np.dot(w2c, point)

    # New we must change from UE4's coordinate system to an "standard"
    # (x, y ,z) -> (y, -z, x)
    # and we remove the fourth componebonent also
    point_camera = [point_camera[1], -point_camera[2], point_camera[0]]

    # now project 3D->2D using the camera matrix
    point_img = np.dot(K, point_camera)
    # normalize
    point_img[0] /= point_img[2]
    point_img[1] /= point_img[2]

    return point_img[0:2]


class Envoy:
    def __init__(self, name, port, tm_port, town_map):
        self.name = name

        # In this tutorial script, we are going to add a vehicle to the simulation
        # and let it drive in autopilot. We will also create a camera attached to
        # that vehicle, and save all the images generated by the camera to disk.

        # try:
        # First of all, we need to create the client that will send the requests
        # to the simulator. Here we'll assume the simulator is accepting
        # requests in the localhost at specific port.
        client = carla.Client('localhost', port)
        client.set_timeout(10.0)

        # Once we have a client we can retrieve the world that is currently
        # running.
        self.world = client.get_world()

        if self.name == 'env_one':
            actor_list = self.world.get_actors()
            print('destroying current actors')
            client.apply_batch([carla.command.DestroyActor(x) for x in actor_list])
            print('done.')

            self.world = client.load_world(town_map)

        print('map:', self.world.get_map().name)

        traffic_manager = client.get_trafficmanager(tm_port)
        traffic_manager.set_random_device_seed(1)

        # Set up the simulator in synchronous mode
        settings = self.world.get_settings()
        settings.synchronous_mode = True # Enables synchronous mode
        # if self.name == 'env_one':
        traffic_manager.set_synchronous_mode(True)
        settings.fixed_delta_seconds = 0.05
        self.world.apply_settings(settings)

        # The world contains the list blueprints that we can use for adding new
        # actors into the simulation.
        blueprint_library = self.world.get_blueprint_library()

        # Now let's filter all the blueprints of type 'vehicle' and choose one
        # at random.
        bp = random.choice(blueprint_library.filter('vehicle'))

        # A blueprint contains the list of attributes that define a vehicle's
        # instance, we can read them and modify some of them. For instance,
        # let's randomize its color.
        if bp.has_attribute('color'):
            color = random.choice(bp.get_attribute('color').recommended_values)
            bp.set_attribute('color', color)

        # Now we need to give an initial transform to the vehicle. We choose a
        # random transform from the list of recommended spawn points of the map.
        transform = random.choice(self.world.get_map().get_spawn_points())
        if self.name == 'env_two':
            np.random.seed(1)
            transform = np.random.choice(self.world.get_map().get_spawn_points())

        # So let's tell the world to spawn the vehicle.
        self.vehicle = self.world.spawn_actor(bp, transform)

        batch = []
        SetAutopilot = carla.command.SetAutopilot
        batch.append(SetAutopilot(self.vehicle, True, traffic_manager.get_port()))

        # It is important to note that the actors we create won't be destroyed
        # unless we call their "destroy" function. If we fail to call "destroy"
        # they will stay in the simulation even after we quit the Python script.
        # For that reason, we are storing all the actors we create so we can
        # destroy them afterwards.
        # actor_list.append(self.vehicle)
        print('created %s' % self.vehicle.type_id)

        # Let's put the vehicle to drive around.
        # vehicle.set_autopilot(True)

        # Let's add now a "rgb" camera attached to the vehicle. Note that the
        # transform we give here is now relative to the vehicle.
        camera_bp = blueprint_library.find('sensor.camera.rgb')
        camera_transform = carla.Transform(carla.Location(z=2))
        self.camera = self.world.spawn_actor(camera_bp, camera_transform, attach_to=self.vehicle)

        # actor_list.append(self.camera)
        print('created %s' % self.camera.type_id)

        # semantic_camera = blueprint_library.find('sensor.camera.semantic_segmentation')
        # self.semantic_camera = self.world.spawn_actor(semantic_camera, camera_transform, attach_to=self.vehicle)

        # print('created %s' % self.semantic_camera.type_id)

        client.apply_batch_sync(batch)
        # self.camera.listen(lambda data: self.image_queue.put((data, 'rgb_camera')))  
        # self.semantic_camera.listen(lambda data: self.image_queue.put((data, 'semantic_camera')))  

        # Get the attributes from the camera
        self.image_w = camera_bp.get_attribute("image_size_x").as_int()
        self.image_h = camera_bp.get_attribute("image_size_y").as_int()
        fov = camera_bp.get_attribute("fov").as_float()

        # Calculate the camera projection matrix to project from 3D -> 2D
        self.K = build_projection_matrix(self.image_w, self.image_h, fov)

        if self.name == 'env_one':
            for i in range(100):
                vehicle_bp = random.choice(blueprint_library.filter('vehicle'))
                npc = self.world.try_spawn_actor(vehicle_bp, random.choice(self.world.get_map().get_spawn_points()))
                if npc:
                    npc.set_autopilot(True, traffic_manager.get_port())
        #     while True:
        #         pass
        
        # Now we register the function that will be called each time the sensor
        # receives an image. In this example we are saving the image to disk.
        #camera.listen(lambda image: image.save_to_disk('_out/%s/%d.png' % (args.name, image.frame)))
        # camera.listen(lambda image: sensor_callback(world, vehicle, image_queue.put, world_2_camera, K, args.name))  
        self.image_queue = queue.Queue()
        self.camera.listen(self.image_queue.put)

        # finally:
        #     print('destroying actors')
        #     self.camera.destroy()
        #     client.apply_batch([carla.command.DestroyActor(x) for x in actor_list])
        #     print('done.')
        #     # Always disable sync mode before the script ends to prevent the server blocking whilst waiting for a tick
        #     settings.synchronous_mode = False
        #     traffic_manager.set_synchronous_mode(False)
        #     time.sleep(0.5)

    def envoy_tick(self, i):
        if self.name == 'env_one':
            self.world.tick()

        image = self.image_queue.get()

        # if (i-1) % 4 != 0:
        #     return
        if image.frame % 2 != 0:
           return
        # if i % 4 == 0:
        #    return
        # if (i-1) % 10 != 0:
        #     return

        # Get the camera matrix 
        world_2_camera = np.array(self.camera.get_transform().get_inverse_matrix())

        dirs = '../_out/' + self.name
        imgs_path = dirs + '/images'
        annot_path = dirs + '/annotations'
        if not os.path.exists(imgs_path):
            os.makedirs(imgs_path)
        if not os.path.exists(annot_path):
            os.makedirs(annot_path)
        # image.save_to_disk(imgs_path + '/%d' % (image.frame) + '.jpg')
        img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))

        # Initialize the exporter
        writer = Writer(annot_path + '/%d' % (image.frame) + '.jpg', self.image_w, self.image_h)

        for npc in self.world.get_actors().filter('*vehicle*'):

            # Filter out the ego vehicle
            if npc.id != self.vehicle.id:

                bb = npc.bounding_box
                dist = npc.get_transform().location.distance(self.vehicle.get_transform().location)

                # Filter for the vehicles within 20m
                if dist < 20:

                # Calculate the dot product between the forward vector
                # of the vehicle and the vector between the vehicle
                # and the other vehicle. We threshold this dot product
                # to limit to drawing bounding boxes IN FRONT OF THE CAMERA
                    forward_vec = self.vehicle.get_transform().get_forward_vector()
                    ray = npc.get_transform().location - self.vehicle.get_transform().location

                    dp = forward_vec.x * ray.x + forward_vec.y * ray.y + forward_vec.z * ray.z

                    # if forward_vec.dot(ray) > 1:
                    if dp > 1:
                        p1 = get_image_point(bb.location, self.K, world_2_camera)
                        verts = [v for v in bb.get_world_vertices(npc.get_transform())]
                        x_max = -10000
                        x_min = 10000
                        y_max = -10000
                        y_min = 10000

                        for vert in verts:
                            p = get_image_point(vert, self.K, world_2_camera)
                            # Find the rightmost vertex
                            if p[0] > x_max:
                                x_max = p[0]
                            # Find the leftmost vertex
                            if p[0] < x_min:
                                x_min = p[0]
                            # Find the highest vertex
                            if p[1] > y_max:
                                y_max = p[1]
                            # Find the lowest  vertex
                            if p[1] < y_min:
                                y_min = p[1]

                        # Add the object to the frame (ensure it is inside the image)
                        if x_min > 0 and x_max < self.image_w and y_min > 0 and y_max < self.image_h: 
                            # Filtering Out Unreliable Detections
                            writer.addObject('vehicle', x_min, y_min, x_max, y_max)
                                           
                        #cv2.line(img, (int(x_min),int(y_min)), (int(x_max),int(y_min)), (0,0,255, 255), 1)
                        #cv2.line(img, (int(x_min),int(y_max)), (int(x_max),int(y_max)), (0,0,255, 255), 1)
                        #cv2.line(img, (int(x_min),int(y_min)), (int(x_min),int(y_max)), (0,0,255, 255), 1)
                        #cv2.line(img, (int(x_max),int(y_min)), (int(x_max),int(y_max)), (0,0,255, 255), 1)  

        # for bb in self.world.get_level_bbs(carla.CityObjectLabel.TrafficLight):

        #     dist = bb.location.distance(self.vehicle.get_transform().location)

        #     if dist < 20:

        #         forward_vec = self.vehicle.get_transform().get_forward_vector()
        #         ray = bb.location - self.vehicle.get_transform().location

        #         dp = forward_vec.x * ray.x + forward_vec.y * ray.y + forward_vec.z * ray.z

        #         if dp > 1:
        #             p1 = get_image_point(bb.location, self.K, world_2_camera)
        #             verts = [v for v in bb.get_world_vertices(carla.Transform())]
        #             x_max = -10000
        #             x_min = 10000
        #             y_max = -10000
        #             y_min = 10000

        #             for vert in verts:
        #                 p = get_image_point(vert, self.K, world_2_camera)
        #                 if p[0] > x_max:
        #                     x_max = p[0]
        #                 if p[0] < x_min:
        #                     x_min = p[0]
        #                 if p[1] > y_max:
        #                     y_max = p[1]
        #                 if p[1] < y_min:
        #                     y_min = p[1]

        #             if x_min > 0 and x_max < self.image_w and y_min > 0 and y_max < self.image_h: 
        #                 writer.addObject('traffic light', x_min, y_min, x_max, y_max)                                           
                    
        #             # cv2.line(img, (int(x_min),int(y_min)), (int(x_max),int(y_min)), (0,0,255, 255), 1)
        #             # cv2.line(img, (int(x_min),int(y_max)), (int(x_max),int(y_max)), (0,0,255, 255), 1)
        #             # cv2.line(img, (int(x_min),int(y_min)), (int(x_min),int(y_max)), (0,0,255, 255), 1)
        #             # cv2.line(img, (int(x_max),int(y_min)), (int(x_max),int(y_max)), (0,0,255, 255), 1)  

        # for bb in self.world.get_level_bbs(carla.CityObjectLabel.TrafficSigns):

        #     dist = bb.location.distance(self.vehicle.get_transform().location)

        #     if dist < 20:

        #         forward_vec = self.vehicle.get_transform().get_forward_vector()
        #         ray = bb.location - self.vehicle.get_transform().location

        #         dp = forward_vec.x * ray.x + forward_vec.y * ray.y + forward_vec.z * ray.z

        #         if dp > 1:
        #             p1 = get_image_point(bb.location, self.K, world_2_camera)
        #             verts = [v for v in bb.get_world_vertices(carla.Transform())]
        #             x_max = -10000
        #             x_min = 10000
        #             y_max = -10000
        #             y_min = 10000

        #             for vert in verts:
        #                 p = get_image_point(vert, self.K, world_2_camera)
        #                 if p[0] > x_max:
        #                     x_max = p[0]
        #                 if p[0] < x_min:
        #                     x_min = p[0]
        #                 if p[1] > y_max:
        #                     y_max = p[1]
        #                 if p[1] < y_min:
        #                     y_min = p[1]

        #             if x_min > 0 and x_max < self.image_w and y_min > 0 and y_max < self.image_h: 
        #                 writer.addObject('traffic sign', x_min, y_min, x_max, y_max)                                           
                    
                    # cv2.line(img, (int(x_min),int(y_min)), (int(x_max),int(y_min)), (0,0,255, 255), 1)
                    # cv2.line(img, (int(x_min),int(y_max)), (int(x_max),int(y_max)), (0,0,255, 255), 1)
                    # cv2.line(img, (int(x_min),int(y_min)), (int(x_min),int(y_max)), (0,0,255, 255), 1)
                    # cv2.line(img, (int(x_max),int(y_min)), (int(x_max),int(y_max)), (0,0,255, 255), 1) 
        
        # Save the bounding boxes in the scene
        writer.save(annot_path + '/%d' % (image.frame) + '.xml')
        # time.sleep(0.05)
        
        cv2.imwrite(imgs_path + '/%d' % (image.frame) + '.jpg', img)

        # if len(os.listdir(imgs_path)) > 100:
        #     img_list = os.listdir(imgs_path)
        #     img_list.sort(key = lambda x: int(x[:-4]))
        #     os.remove(imgs_path + '/' + img_list[0])
        # if len(os.listdir(annot_path)) > 100:
        #     annot_list = os.listdir(annot_path)
        #     annot_list.sort(key = lambda x: int(x[:-4]))
        #     os.remove(annot_path + '/' + annot_list[0])


if __name__ == '__main__':

    # main()
    argparser = argparse.ArgumentParser(description=__doc__)
    argparser.add_argument('--name', help='Name of the generated envoy')
    argparser.add_argument('--port', metavar='P', default=2000, type=int, help='TCP port to listen to (default: 2000)')
    argparser.add_argument(
        '--tm_port',
        metavar='P',
        default=8000,
        type=int,
        help='port to communicate with TM (default: 8000)')        
    argparser.add_argument('--town_map', help='Name of the town map')
    args = argparser.parse_args()
    Envoy(args.name, args.port, args.tm_port, args.town_map)

