#!/usr/bin/env python

# Copyright (c) 2019 Computer Vision Center (CVC) at the Universitat Autonoma de
# Barcelona (UAB).
#
# This work is licensed under the terms of the MIT license.
# For a copy, see <https://opensource.org/licenses/MIT>.

import glob
import os
import sys

try:
    #sys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (
    #    sys.version_info.major,
    #    sys.version_info.minor,
    #    'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])
    sys.path.append(glob.glob('/home/inteluser/Desktop/Shenghong/CARLA_package/CARLA_0.9.11/PythonAPI/carla/dist/carla-0.9.11-py3.7-linux-x86_64.egg')[0])
except IndexError:
    pass

import carla

import random
import time

import argparse

import math
import queue
import numpy as np
import cv2

        
def build_projection_matrix(w, h, fov):
    focal = w / (2.0 * np.tan(fov * np.pi / 360.0))
    K = np.identity(3)
    K[0, 0] = K[1, 1] = focal
    K[0, 2] = w / 2.0
    K[1, 2] = h / 2.0
    return K

def get_image_point(loc, K, w2c):
    # Calculate 2D projection of 3D coordinate

    # Format the input coordinate (loc is a carla.Position object)
    point = np.array([loc.x, loc.y, loc.z, 1])
    # transform to camera coordinates
    point_camera = np.dot(w2c, point)

    # New we must change from UE4's coordinate system to an "standard"
    # (x, y ,z) -> (y, -z, x)
    # and we remove the fourth componebonent also
    point_camera = [point_camera[1], -point_camera[2], point_camera[0]]

    # now project 3D->2D using the camera matrix
    point_img = np.dot(K, point_camera)
    # normalize
    point_img[0] /= point_img[2]
    point_img[1] /= point_img[2]

    return point_img[0:2]

def main():
    argparser = argparse.ArgumentParser(
        description=__doc__)
    argparser.add_argument(
        '--name',
        help='Name of the generated envoy')
    args = argparser.parse_args()
        
    actor_list = []

    # In this tutorial script, we are going to add a vehicle to the simulation
    # and let it drive in autopilot. We will also create a camera attached to
    # that vehicle, and save all the images generated by the camera to disk.

    try:
        # First of all, we need to create the client that will send the requests
        # to the simulator. Here we'll assume the simulator is accepting
        # requests in the localhost at port 2000.
        client = carla.Client('localhost', 2000)
        client.set_timeout(2.0)

        traffic_manager = client.get_trafficmanager(8001)

        # Once we have a client we can retrieve the world that is currently
        # running.
        world = client.get_world()

        # The world contains the list blueprints that we can use for adding new
        # actors into the simulation.
        blueprint_library = world.get_blueprint_library()

        # Now let's filter all the blueprints of type 'vehicle' and choose one
        # at random.
        bp = random.choice(blueprint_library.filter('vehicle'))

        # A blueprint contains the list of attributes that define a vehicle's
        # instance, we can read them and modify some of them. For instance,
        # let's randomize its color.
        if bp.has_attribute('color'):
            color = random.choice(bp.get_attribute('color').recommended_values)
            bp.set_attribute('color', color)

        # Now we need to give an initial transform to the vehicle. We choose a
        # random transform from the list of recommended spawn points of the map.
        transform = random.choice(world.get_map().get_spawn_points())

        # So let's tell the world to spawn the vehicle.
        vehicle = world.spawn_actor(bp, transform)

        batch = []
        SetAutopilot = carla.command.SetAutopilot
        batch.append(SetAutopilot(vehicle, True, traffic_manager.get_port()))

        # It is important to note that the actors we create won't be destroyed
        # unless we call their "destroy" function. If we fail to call "destroy"
        # they will stay in the simulation even after we quit the Python script.
        # For that reason, we are storing all the actors we create so we can
        # destroy them afterwards.
        actor_list.append(vehicle)
        print('created %s' % vehicle.type_id)

        # Let's put the vehicle to drive around.
        # vehicle.set_autopilot(True)

        # Let's add now a "rgb" camera attached to the vehicle. Note that the
        # transform we give here is now relative to the vehicle.
        camera_bp = blueprint_library.find('sensor.camera.rgb')
        camera_transform = carla.Transform(carla.Location(z=2))
        camera = world.spawn_actor(camera_bp, camera_transform, attach_to=vehicle)

        actor_list.append(camera)
        print('created %s' % camera.type_id)

        # Set up the simulator in synchronous mode
        settings = world.get_settings()
        settings.synchronous_mode = True # Enables synchronous mode
        if args.name == 'env_one':
            traffic_manager.set_synchronous_mode(True)
        settings.fixed_delta_seconds = 0.05
        world.apply_settings(settings)

        client.apply_batch_sync(batch)
        
        # Now we register the function that will be called each time the sensor
        # receives an image. In this example we are saving the image to disk.
        #camera.listen(lambda image: image.save_to_disk('_out/%s/%d.png' % (args.name, image.frame)))
        # camera.listen(lambda image: sensor_callback(world, vehicle, image_queue.put, world_2_camera, K, args.name))  
        image_queue = queue.Queue()
        camera.listen(image_queue.put)    

        # Get the attributes from the camera
        image_w = camera_bp.get_attribute("image_size_x").as_int()
        image_h = camera_bp.get_attribute("image_size_y").as_int()
        fov = camera_bp.get_attribute("fov").as_float()

        # Calculate the camera projection matrix to project from 3D -> 2D
        K = build_projection_matrix(image_w, image_h, fov)

        # for i in range(50):
        #     vehicle_bp = random.choice(blueprint_library.filter('vehicle'))
        #     npc = world.try_spawn_actor(vehicle_bp, random.choice(world.get_map().get_spawn_points()))
        #     if npc:
        #         npc.set_autopilot(True)

        while True:
            if args.name == 'env_one':
                world.tick()

            image = image_queue.get()

            img = np.reshape(np.copy(image.raw_data), (image.height, image.width, 4))

            # Get the camera matrix 
            world_2_camera = np.array(camera.get_transform().get_inverse_matrix())

            for npc in world.get_actors().filter('*vehicle*'):

                # Filter out the ego vehicle
                if npc.id != vehicle.id:

                    bb = npc.bounding_box
                    dist = npc.get_transform().location.distance(vehicle.get_transform().location)

                    # Filter for the vehicles within 50m
                    if dist < 50:

                    # Calculate the dot product between the forward vector
                    # of the vehicle and the vector between the vehicle
                    # and the other vehicle. We threshold this dot product
                    # to limit to drawing bounding boxes IN FRONT OF THE CAMERA
                        forward_vec = vehicle.get_transform().get_forward_vector()
                        ray = npc.get_transform().location - vehicle.get_transform().location

                        dp = forward_vec.x * ray.x + forward_vec.y * ray.y + forward_vec.z * ray.z

                        # if forward_vec.dot(ray) > 1:
                        if dp > 1:
                            p1 = get_image_point(bb.location, K, world_2_camera)
                            verts = [v for v in bb.get_world_vertices(npc.get_transform())]
                            x_max = -10000
                            x_min = 10000
                            y_max = -10000
                            y_min = 10000

                            for vert in verts:
                                p = get_image_point(vert, K, world_2_camera)
                                # Find the rightmost vertex
                                if p[0] > x_max:
                                    x_max = p[0]
                                # Find the leftmost vertex
                                if p[0] < x_min:
                                    x_min = p[0]
                                # Find the highest vertex
                                if p[1] > y_max:
                                    y_max = p[1]
                                # Find the lowest  vertex
                                if p[1] < y_min:
                                    y_min = p[1]

                            cv2.line(img, (int(x_min),int(y_min)), (int(x_max),int(y_min)), (0,0,255, 255), 1)
                            cv2.line(img, (int(x_min),int(y_max)), (int(x_max),int(y_max)), (0,0,255, 255), 1)
                            cv2.line(img, (int(x_min),int(y_min)), (int(x_min),int(y_max)), (0,0,255, 255), 1)
                            cv2.line(img, (int(x_max),int(y_min)), (int(x_max),int(y_max)), (0,0,255, 255), 1)    

            dirs = '_out/' + args.name
            if not os.path.exists(dirs):
                os.makedirs(dirs)
            # image.save_to_disk(dirs + '/%d.png' % (image.frame))

            cv2.imwrite(dirs + '/bb_%d.png' % (image.frame), img)
            # cv2.imshow('ImageWindowName',img)
            # if cv2.waitKey(1) == ord('q'):
            #     break

            if len(os.listdir(dirs)) >= 100:
                img_list = os.listdir(dirs)
                img_list.sort(key = lambda x: int(x[3:-4]))
                os.remove(dirs + '/' + img_list[0])
    

    finally:
        print('destroying actors')
        camera.destroy()
        client.apply_batch([carla.command.DestroyActor(x) for x in actor_list])
        print('done.')
        # Always disable sync mode before the script ends to prevent the server blocking whilst waiting for a tick
        settings.synchronous_mode = False
        traffic_manager.set_synchronous_mode(False)
        time.sleep(0.5)


if __name__ == '__main__':

    main()
